{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find value of killed column only where country == â€˜United Statesâ€™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2836\n",
      "2836\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "file_obj = open('C:\\PYTHON/DataScience_Ninza/DataScience-and-ML/Datasets/terrorismData.csv', 'r', encoding='utf8')\n",
    "file_data = csv.DictReader(file_obj, skipinitialspace = True)\n",
    "    \n",
    "killed=[]\n",
    "country=[]\n",
    "for row in file_data:\n",
    "    killed.append(row['Killed'])\n",
    "    country.append(row['Country'])\n",
    "\n",
    "np_killed = np.array(killed)\n",
    "np_country = np.array(country)\n",
    "\n",
    "np_killed[np_killed == ''] = '0.0'\n",
    "\n",
    "# Convert into float\n",
    "np_killed = np.array(np_killed, dtype=float)\n",
    "\n",
    "# convert into int\n",
    "np_killed = np.array(np_killed, dtype=int)\n",
    "\n",
    "bool_arr = np_country=='United States'\n",
    "ans = np_killed[bool_arr]\n",
    "for i in ans:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find total number of people killed from USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3771\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "file_obj = open('C:\\PYTHON/DataScience_Ninza/DataScience-and-ML/Datasets/terrorismData.csv', encoding='utf8') # by default is readmode\n",
    "file_data = csv.DictReader(file_obj, skipinitialspace = True)\n",
    "    \n",
    "killed=[]\n",
    "country=[]\n",
    "for row in file_data:\n",
    "    killed.append(row['Killed'])\n",
    "    country.append(row['Country'])\n",
    "\n",
    "np_killed = np.array(killed)\n",
    "np_country = np.array(country)\n",
    "\n",
    "# Remove empty spaces\n",
    "np_killed[np_killed == ''] = '0.0'\n",
    "\n",
    "# Convert into float bcz we can't directly convert '2.6' in int\n",
    "np_killed = np.array(np_killed, dtype=float)\n",
    "\n",
    "# convert into int\n",
    "np_killed = np.array(np_killed, dtype=int)\n",
    "\n",
    "bool_arr = np_country=='United States'\n",
    "ans = np_killed[bool_arr]\n",
    "print(np.sum(ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the number of attack held between day 10 and day 20?(ignoring the year and month)(including both day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66330\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "file_obj = open('C:\\PYTHON/DataScience_Ninza/DataScience-and-ML/Datasets/terrorismData.csv', encoding='utf8') # by default is readmode\n",
    "data = csv.DictReader(file_obj, skipinitialspace=True)\n",
    "\n",
    "day = []\n",
    "for row in data:\n",
    "    day.append(row['Day'])\n",
    "\n",
    "np_day = np.array(day, dtype=int)\n",
    "\n",
    "ans = np_day[(np_day>=10) & (np_day<=20)]\n",
    "print(len(ans))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the number of attack held between 1 Jan 2010 and 31 Jan 2010?(including both date).\n",
    "## Note Ignore the case where day is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "file_obj = open('C:\\PYTHON/DataScience_Ninza/DataScience-and-ML/Datasets/terrorismData.csv', encoding='utf8') # by default is readmode\n",
    "data = csv.DictReader(file_obj, skipinitialspace=True)\n",
    "\n",
    "day = []\n",
    "year = []\n",
    "month = []\n",
    "for row in data:\n",
    "    day.append(row['Day'])\n",
    "    year.append(row['Year'])\n",
    "    month.append(row['Month'])\n",
    "\n",
    "np_day = np.array(day, dtype=int)\n",
    "np_year = np.array(year, dtype=int)\n",
    "np_month = np.array(month, dtype=int)\n",
    "\n",
    "ans = np_day[(np_year==2010) & (np_month==1) & (np_day>=1) & (np_day<=31) & (np_day!=0)]\n",
    "print(len(ans))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we knew the Kargil ( in Jammu and Kashmir) War that took place between May 1999 and July 1999 (3 Months) ,so there was a huge conflict in Kashmir Valley during this period.\n",
    "### In this dataset, there is no information regarding the war between the two countries to find out the casualty during the war.\n",
    "### So find out the attack in this period in which maximum casualties happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 Kargil District Separatists\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "file_obj = open('C:/PYTHON/DataScience_Ninza/DataScience-and-ML/Datasets/terrorismData.csv', encoding='utf8') # by default is readmode\n",
    "data = csv.DictReader(file_obj, skipinitialspace=True)\n",
    "\n",
    "year = []\n",
    "month = []\n",
    "killed = []\n",
    "wounded = []\n",
    "city = []\n",
    "group = []\n",
    "\n",
    "for row in data:\n",
    "    m, y, s = int(row['Month']), int(row['Year']), row['State']  # Chk filters here otherwise if array becomes Large it cause TLE  \n",
    "    if (y==1999) and (m>=5) and (m<=7) and (s=='Jammu and Kashmir'):\n",
    "        killed.append(row['Killed'])\n",
    "        year.append(y)\n",
    "        month.append(m)\n",
    "        wounded.append(row['Wounded'])\n",
    "        city.append(row['City'])\n",
    "        group.append(row['Group'])\n",
    "\n",
    "# Make numpy array\n",
    "np_killed = np.array(killed)\n",
    "np_wounded = np.array(wounded)\n",
    "np_city = np.array(city)\n",
    "np_group = np.array(group)\n",
    "\n",
    "\n",
    "np_killed[np_killed==''] = '0.0'\n",
    "np_wounded[np_wounded==''] = '0.0'\n",
    "\n",
    "# Convert into float\n",
    "np_killed = np.array(np_killed, dtype = float)\n",
    "np_wounded = np.array(np_wounded, dtype = float)\n",
    "\n",
    "#convert into int\n",
    "np_killed = np.array(np_killed, dtype = int)\n",
    "np_wounded = np.array(np_wounded, dtype = int)\n",
    "np_year = np.array(year, dtype=int)\n",
    "np_month = np.array(month, dtype=int)\n",
    "\n",
    "np_casuality = np_killed + np_wounded\n",
    "\n",
    "\n",
    "ind = np.argmax(np_casuality)\n",
    "print(np_casuality[ind], np_city[ind], np_group[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the casualty in the Red Corridor States ? Mainly Red corridor states include Jharkhand, Odisha, Andhra Pradesh, and Chhattisgarh.\n",
    "## Note: Casualty=Killed +Wounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5628\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "file_obj = open('C:\\PYTHON/DataScience_Ninza/DataScience-and-ML/Datasets/terrorismData.csv', encoding='utf8') # by default is readmode\n",
    "data = csv.DictReader(file_obj, skipinitialspace=True)\n",
    "\n",
    "killed = []\n",
    "wounded = []\n",
    "state = []\n",
    "for row in data:\n",
    "    killed.append(row['Killed'])\n",
    "    wounded.append(row['Wounded'])\n",
    "    state.append(row['State'])\n",
    "\n",
    "# Make numpy array\n",
    "np_killed = np.array(killed)\n",
    "np_wounded = np.array(wounded)\n",
    "np_state = np.array(state)\n",
    "\n",
    "np_killed[np_killed==''] = '0.0'\n",
    "np_wounded[np_wounded==''] = '0.0'\n",
    "\n",
    "# Convert into float\n",
    "np_killed = np.array(np_killed, dtype = float)\n",
    "np_wounded = np.array(np_wounded, dtype = float)\n",
    "\n",
    "#convert into int\n",
    "np_killed = np.array(np_killed, dtype = int)\n",
    "np_wounded = np.array(np_wounded, dtype = int)\n",
    "\n",
    "np_casuality = np_killed + np_wounded\n",
    "\n",
    "# Find casualty in the Red Corridor States\n",
    "bool_arr = (np_state == 'Jharkhand') | (np_state == 'Odisha') | (np_state == 'Andhra Pradesh') | (np_state == 'Chhattisgarh')\n",
    "casuality = np_casuality[bool_arr]\n",
    "print(np.sum(casuality))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find top 5 Indian Cities which has most number of casualties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Srinagar 3134\n",
      "New Delhi 2095\n",
      "Mumbai 2016\n",
      "Jammu 1119\n",
      "Guwahati 822\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "file_obj = open('C:\\PYTHON/DataScience_Ninza/DataScience-and-ML/Datasets/terrorismData.csv', encoding='utf8') # by default is readmode\n",
    "data = csv.DictReader(file_obj, skipinitialspace=True)\n",
    "\n",
    "# bcz it is possible that same city is present many times. So to add casulities of all we use dictionary\n",
    "city_cas = {}\n",
    "\n",
    "for row in data:\n",
    "    if row['Country'] == 'India' and row['City'] != 'Unknown': # bcz we have to ignore unknown cities\n",
    "        killed = row['Killed']\n",
    "        wounded = row['Wounded']\n",
    "        city = row['City']\n",
    "        \n",
    "        if killed == '':\n",
    "            killed = 0\n",
    "        else:\n",
    "            killed = int(float(killed))\n",
    "            \n",
    "        if wounded == '':\n",
    "            wounded = 0\n",
    "        else:\n",
    "            wounded = int(float(wounded))\n",
    "            \n",
    "        city_cas[city] = city_cas.get(city, 0) + killed + wounded # Store total casualities of a particular city\n",
    "    \n",
    "city = []\n",
    "casuality = []\n",
    "for key in city_cas:\n",
    "    city.append(key)\n",
    "    casuality.append(city_cas[key])\n",
    "    \n",
    "# Make numpy array\n",
    "np_casuality = np.array(casuality)\n",
    "np_city = np.array(city)\n",
    "\n",
    "# Now we have to sort according to casuality\n",
    "indices = np.argsort(np_casuality) #It gives indices on basis of increasing order of casualities\n",
    "\n",
    "# Sort acc to decreasing order\n",
    "cas = np_casuality[indices][::-1] # [::-1] bcz we went to sort in reverse order\n",
    "cities = np_city[indices][::-1]\n",
    "\n",
    "\n",
    "# Print top 5 Indian where city!='Unknown'\n",
    "for i in range(5):\n",
    "    print(cities[i], cas[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the most frequent day of attack in a terrorismDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 6500\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "obj = open('C:/PYTHON/DataScience_Ninza/DataScience-and-ML/Datasets/terrorismData.csv', encoding='utf8')\n",
    "data = csv.DictReader(obj, skipinitialspace=True)\n",
    "\n",
    "day = []\n",
    "for row in data:\n",
    "    day.append(row['Day'])\n",
    "    \n",
    "np_day = np.array(day, dtype=int)\n",
    "\n",
    "unique_days, counts = np.unique(np_day, return_counts=True) #rteurn_counts=True krne se count bhi saath hi mil jjaega for ech distinct element\n",
    "freq = dict(zip(unique_days, counts))\n",
    "\n",
    "day_freq = np_day[0] #frequent day\n",
    "count_freq = freq[np_day[0]] #frequent count\n",
    "\n",
    "for key in freq:\n",
    "    if(count_freq < freq[key]):\n",
    "        count_freq = freq[key]\n",
    "        day_freq = key\n",
    "print(day_freq, count_freq)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### argsort() examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3)\n",
      "[[ 4  1 -2]\n",
      " [ 2  2  8]\n",
      " [ 3  2 -7]\n",
      " [ 1  4  3]]\n",
      "[2 0 3 1]\n",
      "[[ 3  2 -7]\n",
      " [ 4  1 -2]\n",
      " [ 1  4  3]\n",
      " [ 2  2  8]]\n"
     ]
    }
   ],
   "source": [
    "# sort on basis of 3rd column\n",
    "a = np.array([[4,1, -2], [2,2,8], [3,2,-7], [1,4,3]])\n",
    "print(a.shape)\n",
    "print(a)\n",
    "b  =np.argsort(a[:,2])\n",
    "c = a[b]\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 23  -6   0 867  87  53]\n",
      "[3 4 5 0 2 1]\n",
      "[867  87  53  23   0  -6]\n"
     ]
    }
   ],
   "source": [
    "#Sort in descending order\n",
    "a = np.array([23,-6,0,867,87,53])\n",
    "print(a)\n",
    "b = np.argsort(a)[::-1]\n",
    "print(b)\n",
    "c = a[b]\n",
    "print(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
