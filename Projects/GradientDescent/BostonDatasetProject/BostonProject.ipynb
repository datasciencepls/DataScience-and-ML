{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boston dataset is one of the datasets available in sklearn.\n",
    "You are given a Training dataset csv file with X train and Y train data. As studied in lecture, your task is to come up with Gradient Descent algorithm and thus predictions for the test dataset given.\n",
    "Your task is to:\n",
    "1. Code Gradient Descent for N features and come with predictions.\n",
    "2. Try and test with various combinations of learning rates and number of iterations.\n",
    "3. Try using Feature Scaling, and see if it helps you in getting better results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient(X_train, Y_train, learning_rate, coeff):\n",
    "    n = len(X_train[0]) # num_features and last is 1 ; last 1 bcz we calculate c(intercept) in this array also\n",
    "    coefficients = np.zeros(n) # [m1, m2, m3, ... mn, m(n+1)] where m(n+1) is c\n",
    "    M = len(X_train)\n",
    "    \n",
    "    for i in range(M):\n",
    "        x = X_train[i]\n",
    "        y = Y_train[i]\n",
    "        for j in range(n):\n",
    "            coefficients[j] += (-2/M)*(y - (coeff*x).sum())*x[j]\n",
    "    new_coeff = coeff - learning_rate*coefficients\n",
    "    return new_coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " 2nd Way\n",
    "def cost(X_train, Y_train, coeff):\n",
    "    total_cost = 0\n",
    "    M = len(X_train)\n",
    "    for i in range(M):\n",
    "        x = X_train[i]\n",
    "        y = Y_train[i]\n",
    "        total_cost += (1/M)*( (y - (coeff*x).sum())**2 )\n",
    "    return total_cost\n",
    "'''\n",
    "\n",
    "def cost(X_train, Y_train, coeff):\n",
    "    return ((Y_train - np.sum(coeff*X_train, axis = 1))**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(X_train, Y_train, learning_rate, num_iterations):\n",
    "    # append column of 1's in X_train\n",
    "    ones_col = np.ones(len(X_train)).reshape(-1,1) # reshape bcz we want column of 1's\n",
    "    X_train = np.append(X_train, ones_col, axis=1)\n",
    "    \n",
    "    n = len(X_train[0]) # num_features+1 ; +1 bcz we calculate c(intercept) in this array also\n",
    "    \n",
    "    # choose random value for coefficients lets say 0\n",
    "    coefficients = np.zeros(n) # [m1, m2, m3, ... mn, m(n+1)] where m(n+1) is c\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        coefficients = step_gradient(X_train, Y_train, learning_rate, coefficients)\n",
    "        \n",
    "        # printing cost after every iteration, so that we can see that after which iteration cost is not decreasing much\n",
    "        print(\"After iteration \",i+1, \"Cost is:\", cost(X_train, Y_train, coefficients))\n",
    "        \n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def predictions(X_test, m, c):\n",
    "    M = len(X_test)\n",
    "    y_pred = np.zeros(M)\n",
    "    for i in range(M):\n",
    "        x = X_test[i]\n",
    "        y_pred[i] += ((m*x).sum()+c)\n",
    "    return y_pred\n",
    "'''\n",
    "def predictions(X_test, m, c):\n",
    "    return (np.sum(m*X_test, axis = 1)+c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run function which loads data apply feature scaling on it and calls gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    training_data = np.genfromtxt('boston_traindata.csv', delimiter=',')\n",
    "    X_train = training_data[:, :-1]\n",
    "    Y_train = training_data[:, -1]\n",
    "    \n",
    "    X_test = np.genfromtxt('boston_testdata.csv', delimiter=',')\n",
    "    \n",
    "    \n",
    "    # Add more features in both X_train and X_test of 2 degree i.e square of each col to make complex boundries in order to reduce cost\n",
    "    num_col = X_train.shape[1]\n",
    "    for i in range(num_col):\n",
    "        ith_col = X_train[:, i]\n",
    "        new_col = (ith_col*ith_col).reshape(-1,1) # square of each column\n",
    "        X_train = np.append(X_train, new_col, axis = 1)\n",
    "        \n",
    "        ith_test_col = X_test[:, i]\n",
    "        squared_test_col = (ith_test_col*ith_test_col).reshape(-1, 1)\n",
    "        X_test = np.append(X_test, squared_test_col, axis=1)\n",
    "    \n",
    "    # Appply feature scaling\n",
    "    scaler = preprocessing.StandardScaler() # create scaler object\n",
    "    scaler.fit(X_train)\n",
    "    transformed_X_train = scaler.transform(X_train)\n",
    "    transformed_X_test = scaler.transform(X_test)\n",
    "    \n",
    "    learning_rate = 0.035\n",
    "    num_iterations = 200\n",
    "    parameters = gd(transformed_X_train, Y_train, learning_rate, num_iterations)\n",
    "    m = parameters[:-1]\n",
    "    c = parameters[-1]\n",
    "    #print(m, c, sep=\"\\n\")\n",
    "    \n",
    "    # call prediction\n",
    "    pred = predictions(transformed_X_test, m, c).reshape(-1,1)\n",
    "    # Rounding off upto 5 decimal places\n",
    "    #pred = np.round(pred, decimals=5)\n",
    "    # Save Predictions\n",
    "    np.savetxt('predictions2.csv', pred, delimiter=',')\n",
    "    print(pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### call run function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After iteration  1 Cost is: 498.9022789927327\n",
      "After iteration  2 Cost is: 428.19899202751134\n",
      "After iteration  3 Cost is: 370.5713724040005\n",
      "After iteration  4 Cost is: 321.7616606315328\n",
      "After iteration  5 Cost is: 280.0070774429033\n",
      "After iteration  6 Cost is: 244.17393591277963\n",
      "After iteration  7 Cost is: 213.37576705890766\n",
      "After iteration  8 Cost is: 186.87798071258754\n",
      "After iteration  9 Cost is: 164.061529178858\n",
      "After iteration  10 Cost is: 144.4014235688864\n",
      "After iteration  11 Cost is: 127.45088408232984\n",
      "After iteration  12 Cost is: 112.82873857549718\n",
      "After iteration  13 Cost is: 100.20914417671267\n",
      "After iteration  14 Cost is: 89.3131031645834\n",
      "After iteration  15 Cost is: 79.90140532512179\n",
      "After iteration  16 Cost is: 71.76872161291999\n",
      "After iteration  17 Cost is: 64.73863697138505\n",
      "After iteration  18 Cost is: 58.65945589249208\n",
      "After iteration  19 Cost is: 53.40064841761528\n",
      "After iteration  20 Cost is: 48.84983020444732\n",
      "After iteration  21 Cost is: 44.91019027475349\n",
      "After iteration  22 Cost is: 41.498295680092916\n",
      "After iteration  23 Cost is: 38.542214682889394\n",
      "After iteration  24 Cost is: 35.97990993936789\n",
      "After iteration  25 Cost is: 33.75786116255009\n",
      "After iteration  26 Cost is: 31.829883259304953\n",
      "After iteration  27 Cost is: 30.156111289340334\n",
      "After iteration  28 Cost is: 28.70212802296435\n",
      "After iteration  29 Cost is: 27.43821355961946\n",
      "After iteration  30 Cost is: 26.33869955088086\n",
      "After iteration  31 Cost is: 25.38141315971735\n",
      "After iteration  32 Cost is: 24.54719806939233\n",
      "After iteration  33 Cost is: 23.819501700090697\n",
      "After iteration  34 Cost is: 23.184019355430397\n",
      "After iteration  35 Cost is: 22.6283873502374\n",
      "After iteration  36 Cost is: 22.141918302857192\n",
      "After iteration  37 Cost is: 21.715372740815024\n",
      "After iteration  38 Cost is: 21.340761993511048\n",
      "After iteration  39 Cost is: 21.011178051264917\n",
      "After iteration  40 Cost is: 20.720646674340905\n",
      "After iteration  41 Cost is: 20.464000553657684\n",
      "After iteration  42 Cost is: 20.236769769420476\n",
      "After iteration  43 Cost is: 20.035087175647046\n",
      "After iteration  44 Cost is: 19.855606666593577\n",
      "After iteration  45 Cost is: 19.695432563151396\n",
      "After iteration  46 Cost is: 19.552058599957665\n",
      "After iteration  47 Cost is: 19.42331520284762\n",
      "After iteration  48 Cost is: 19.307323926157217\n",
      "After iteration  49 Cost is: 19.20245807435222\n",
      "After iteration  50 Cost is: 19.107308666011324\n",
      "After iteration  51 Cost is: 19.020655013324404\n",
      "After iteration  52 Cost is: 18.941439289550598\n",
      "After iteration  53 Cost is: 18.868744542520414\n",
      "After iteration  54 Cost is: 18.801775686153118\n",
      "After iteration  55 Cost is: 18.739843065723125\n",
      "After iteration  56 Cost is: 18.682348247643848\n",
      "After iteration  57 Cost is: 18.628771732049085\n",
      "After iteration  58 Cost is: 18.578662327474394\n",
      "After iteration  59 Cost is: 18.531627962366\n",
      "After iteration  60 Cost is: 18.487327738741136\n",
      "After iteration  61 Cost is: 18.445465059751278\n",
      "After iteration  62 Cost is: 18.405781685730545\n",
      "After iteration  63 Cost is: 18.36805259303643\n",
      "After iteration  64 Cost is: 18.332081527032873\n",
      "After iteration  65 Cost is: 18.297697155293157\n",
      "After iteration  66 Cost is: 18.26474973982715\n",
      "After iteration  67 Cost is: 18.233108258136653\n",
      "After iteration  68 Cost is: 18.202657912409705\n",
      "After iteration  69 Cost is: 18.173297974381505\n",
      "After iteration  70 Cost is: 18.144939920493133\n",
      "After iteration  71 Cost is: 18.117505818119\n",
      "After iteration  72 Cost is: 18.090926928942352\n",
      "After iteration  73 Cost is: 18.06514250014709\n",
      "After iteration  74 Cost is: 18.040098718061575\n",
      "After iteration  75 Cost is: 18.015747802320284\n",
      "After iteration  76 Cost is: 17.992047221575262\n",
      "After iteration  77 Cost is: 17.968959014353416\n",
      "After iteration  78 Cost is: 17.946449200873275\n",
      "After iteration  79 Cost is: 17.92448727355215\n",
      "After iteration  80 Cost is: 17.90304575559261\n",
      "After iteration  81 Cost is: 17.88209981847088\n",
      "After iteration  82 Cost is: 17.861626950389592\n",
      "After iteration  83 Cost is: 17.841606668829556\n",
      "After iteration  84 Cost is: 17.822020271262335\n",
      "After iteration  85 Cost is: 17.802850618887344\n",
      "After iteration  86 Cost is: 17.78408194895056\n",
      "After iteration  87 Cost is: 17.765699711801876\n",
      "After iteration  88 Cost is: 17.747690429366536\n",
      "After iteration  89 Cost is: 17.730041572155045\n",
      "After iteration  90 Cost is: 17.712741452323563\n",
      "After iteration  91 Cost is: 17.695779130632776\n",
      "After iteration  92 Cost is: 17.679144335443038\n",
      "After iteration  93 Cost is: 17.662827392135128\n",
      "After iteration  94 Cost is: 17.646819161562604\n",
      "After iteration  95 Cost is: 17.631110986329936\n",
      "After iteration  96 Cost is: 17.6156946438529\n",
      "After iteration  97 Cost is: 17.600562305298173\n",
      "After iteration  98 Cost is: 17.585706499620848\n",
      "After iteration  99 Cost is: 17.571120082023498\n",
      "After iteration  100 Cost is: 17.55679620625153\n",
      "After iteration  101 Cost is: 17.542728300218204\n",
      "After iteration  102 Cost is: 17.528910044520764\n",
      "After iteration  103 Cost is: 17.515335353468068\n",
      "After iteration  104 Cost is: 17.501998358290937\n",
      "After iteration  105 Cost is: 17.48889339225064\n",
      "After iteration  106 Cost is: 17.476014977399046\n",
      "After iteration  107 Cost is: 17.463357812776845\n",
      "After iteration  108 Cost is: 17.450916763865013\n",
      "After iteration  109 Cost is: 17.4386868531292\n",
      "After iteration  110 Cost is: 17.426663251518164\n",
      "After iteration  111 Cost is: 17.414841270795932\n",
      "After iteration  112 Cost is: 17.40321635660329\n",
      "After iteration  113 Cost is: 17.391784082158136\n",
      "After iteration  114 Cost is: 17.380540142516093\n",
      "After iteration  115 Cost is: 17.36948034932341\n",
      "After iteration  116 Cost is: 17.358600626002843\n",
      "After iteration  117 Cost is: 17.347897003321272\n",
      "After iteration  118 Cost is: 17.337365615294303\n",
      "After iteration  119 Cost is: 17.32700269538926\n",
      "After iteration  120 Cost is: 17.316804572992552\n",
      "After iteration  121 Cost is: 17.30676767011232\n",
      "After iteration  122 Cost is: 17.296888498290578\n",
      "After iteration  123 Cost is: 17.287163655702695\n",
      "After iteration  124 Cost is: 17.277589824424634\n",
      "After iteration  125 Cost is: 17.268163767851156\n",
      "After iteration  126 Cost is: 17.258882328249907\n",
      "After iteration  127 Cost is: 17.249742424438665\n",
      "After iteration  128 Cost is: 17.24074104957412\n",
      "After iteration  129 Cost is: 17.231875269042387\n",
      "After iteration  130 Cost is: 17.223142218442355\n",
      "After iteration  131 Cost is: 17.214539101654303\n",
      "After iteration  132 Cost is: 17.20606318898674\n",
      "After iteration  133 Cost is: 17.197711815395753\n",
      "After iteration  134 Cost is: 17.18948237877129\n",
      "After iteration  135 Cost is: 17.18137233828579\n",
      "After iteration  136 Cost is: 17.173379212800935\n",
      "After iteration  137 Cost is: 17.1655005793288\n",
      "After iteration  138 Cost is: 17.157734071544006\n",
      "After iteration  139 Cost is: 17.15007737834395\n",
      "After iteration  140 Cost is: 17.142528242454368\n",
      "After iteration  141 Cost is: 17.13508445907782\n",
      "After iteration  142 Cost is: 17.12774387458292\n",
      "After iteration  143 Cost is: 17.12050438523225\n",
      "After iteration  144 Cost is: 17.11336393594729\n",
      "After iteration  145 Cost is: 17.106320519108518\n",
      "After iteration  146 Cost is: 17.0993721733893\n",
      "After iteration  147 Cost is: 17.092516982622108\n",
      "After iteration  148 Cost is: 17.08575307469584\n",
      "After iteration  149 Cost is: 17.079078620482964\n",
      "After iteration  150 Cost is: 17.072491832795414\n",
      "After iteration  151 Cost is: 17.06599096536824\n",
      "After iteration  152 Cost is: 17.059574311870005\n",
      "After iteration  153 Cost is: 17.053240204939005\n",
      "After iteration  154 Cost is: 17.046987015244508\n",
      "After iteration  155 Cost is: 17.04081315057219\n",
      "After iteration  156 Cost is: 17.03471705493297\n",
      "After iteration  157 Cost is: 17.028697207694584\n",
      "After iteration  158 Cost is: 17.02275212273517\n",
      "After iteration  159 Cost is: 17.01688034761823\n",
      "After iteration  160 Cost is: 17.0110804627883\n",
      "After iteration  161 Cost is: 17.005351080786838\n",
      "After iteration  162 Cost is: 16.99969084548764\n",
      "After iteration  163 Cost is: 16.994098431351286\n",
      "After iteration  164 Cost is: 16.988572542698126\n",
      "After iteration  165 Cost is: 16.983111912999266\n",
      "After iteration  166 Cost is: 16.977715304184997\n",
      "After iteration  167 Cost is: 16.972381505970365\n",
      "After iteration  168 Cost is: 16.967109335197254\n",
      "After iteration  169 Cost is: 16.96189763519266\n",
      "After iteration  170 Cost is: 16.95674527514265\n",
      "After iteration  171 Cost is: 16.9516511494817\n",
      "After iteration  172 Cost is: 16.946614177296876\n",
      "After iteration  173 Cost is: 16.941633301746585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After iteration  174 Cost is: 16.9367074894935\n",
      "After iteration  175 Cost is: 16.931835730151246\n",
      "After iteration  176 Cost is: 16.92701703574453\n",
      "After iteration  177 Cost is: 16.922250440182417\n",
      "After iteration  178 Cost is: 16.91753499874433\n",
      "After iteration  179 Cost is: 16.912869787578497\n",
      "After iteration  180 Cost is: 16.90825390321258\n",
      "After iteration  181 Cost is: 16.903686462076067\n",
      "After iteration  182 Cost is: 16.899166600034263\n",
      "After iteration  183 Cost is: 16.894693471933472\n",
      "After iteration  184 Cost is: 16.89026625115716\n",
      "After iteration  185 Cost is: 16.885884129192835\n",
      "After iteration  186 Cost is: 16.881546315209313\n",
      "After iteration  187 Cost is: 16.877252035644133\n",
      "After iteration  188 Cost is: 16.87300053380101\n",
      "After iteration  189 Cost is: 16.86879106945676\n",
      "After iteration  190 Cost is: 16.864622918477817\n",
      "After iteration  191 Cost is: 16.860495372445897\n",
      "After iteration  192 Cost is: 16.856407738292578\n",
      "After iteration  193 Cost is: 16.852359337942744\n",
      "After iteration  194 Cost is: 16.84834950796641\n",
      "After iteration  195 Cost is: 16.844377599239035\n",
      "After iteration  196 Cost is: 16.840442976609836\n",
      "After iteration  197 Cost is: 16.83654501857807\n",
      "After iteration  198 Cost is: 16.83268311697705\n",
      "After iteration  199 Cost is: 16.828856676665666\n",
      "After iteration  200 Cost is: 16.8250651152273\n",
      "[[13.99573822]\n",
      " [28.51488658]\n",
      " [22.66130481]\n",
      " [24.2673586 ]\n",
      " [19.2837082 ]\n",
      " [12.56888981]\n",
      " [27.05335165]\n",
      " [22.88912993]\n",
      " [18.91937431]\n",
      " [23.13412383]\n",
      " [24.92054224]\n",
      " [17.18417796]\n",
      " [18.77390092]\n",
      " [18.82307929]\n",
      " [49.92239715]\n",
      " [23.59844615]\n",
      " [24.20614029]\n",
      " [26.09050473]\n",
      " [19.21918114]\n",
      " [31.39975718]\n",
      " [21.49662669]\n",
      " [24.10241763]\n",
      " [35.00432345]\n",
      " [35.57802401]\n",
      " [32.93418068]\n",
      " [16.91190121]\n",
      " [22.1431202 ]\n",
      " [31.48128502]\n",
      " [23.18794447]\n",
      " [31.8262807 ]\n",
      " [16.30655864]\n",
      " [25.59579991]\n",
      " [23.16259638]\n",
      " [24.00071285]\n",
      " [13.64201397]\n",
      " [27.6829775 ]\n",
      " [24.80394467]\n",
      " [19.03357184]\n",
      " [22.92475947]\n",
      " [10.15289091]\n",
      " [17.2238198 ]\n",
      " [26.31642913]\n",
      " [29.57618152]\n",
      " [19.41423887]\n",
      " [18.48512011]\n",
      " [13.29544011]\n",
      " [47.26885151]\n",
      " [24.14543407]\n",
      " [31.5574372 ]\n",
      " [14.55165488]\n",
      " [16.23358448]\n",
      " [41.5087463 ]\n",
      " [15.27932374]\n",
      " [20.09945538]\n",
      " [15.23293038]\n",
      " [21.54323084]\n",
      " [16.88290425]\n",
      " [22.40320607]\n",
      " [14.57968936]\n",
      " [15.17847525]\n",
      " [11.89309037]\n",
      " [29.25266011]\n",
      " [22.78810449]\n",
      " [25.58218689]\n",
      " [16.19662215]\n",
      " [15.57857765]\n",
      " [34.50121552]\n",
      " [15.44992067]\n",
      " [25.3395253 ]\n",
      " [21.4446284 ]\n",
      " [28.32478632]\n",
      " [25.94456691]\n",
      " [15.0360088 ]\n",
      " [12.12373265]\n",
      " [36.40923589]\n",
      " [23.45870515]\n",
      " [27.20161731]\n",
      " [26.68724042]\n",
      " [14.77155143]\n",
      " [32.1522209 ]\n",
      " [16.99423384]\n",
      " [21.34784114]\n",
      " [22.25164091]\n",
      " [12.68213826]\n",
      " [15.56187065]\n",
      " [31.95697208]\n",
      " [24.04664507]\n",
      " [12.50625488]\n",
      " [20.91467008]\n",
      " [18.0788622 ]\n",
      " [21.1521    ]\n",
      " [18.61998429]\n",
      " [18.47231039]\n",
      " [11.99982256]\n",
      " [20.70805823]\n",
      " [23.357889  ]\n",
      " [42.48252064]\n",
      " [18.92483825]\n",
      " [33.89377812]\n",
      " [25.20157075]\n",
      " [29.10298848]\n",
      " [20.81368922]\n",
      " [23.38494958]\n",
      " [31.28216127]\n",
      " [14.53054247]\n",
      " [24.81341128]\n",
      " [20.77644601]\n",
      " [38.53775339]\n",
      " [23.15694225]\n",
      " [14.72798076]\n",
      " [25.83761801]\n",
      " [16.96282675]\n",
      " [14.13380314]\n",
      " [18.8443701 ]\n",
      " [39.26473887]\n",
      " [19.88270936]\n",
      " [20.15206501]\n",
      " [23.85544877]\n",
      " [21.7094487 ]\n",
      " [17.13893346]\n",
      " [13.62366147]\n",
      " [34.894977  ]\n",
      " [21.55009602]\n",
      " [23.00402966]\n",
      " [21.84801867]\n",
      " [19.37151392]\n",
      " [13.15016855]]\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
