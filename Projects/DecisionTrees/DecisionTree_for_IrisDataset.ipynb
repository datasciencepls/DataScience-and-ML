{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Iris data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "features = iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Store feature names and its corresponding indices initially. Otherwise in every recusive call we need to make new X ndarray\n",
    "    which increases complexity.\n",
    "'''\n",
    "feature_indices = {}\n",
    "for i in range(len(features)):\n",
    "    feature_indices[features[i]] = feature_indices.get(features[i],0) + i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_node(Y):\n",
    "    total_freq = len(Y)\n",
    "\n",
    "    # store different classes possible with its frequency\n",
    "    class_freq = {}\n",
    "    for c in Y:\n",
    "        class_freq[c] = class_freq.get(c,0) + 1\n",
    "        \n",
    "    # Now we calculate entropy or info required\n",
    "    info_req =  0\n",
    "    for k in class_freq:\n",
    "        prob_k = class_freq[k]/total_freq\n",
    "        if prob_k!=0:\n",
    "            info_req += (-1 * prob_k * np.log2(prob_k))\n",
    "    return info_req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate entropy(info required) after spliting on basis of feature having index f_i\n",
    "\n",
    "def info_fi(X, Y, f_i, boundry):\n",
    "    entropy_after_split = np.float(0)\n",
    "    # feature on which we goona split\n",
    "    feature = X[:, f_i] # feature corresponds to index f_i\n",
    "    Y_left = Y[np.where(feature <= boundry)]\n",
    "    Y_right = Y[np.where(feature > boundry)]\n",
    "    \n",
    "    if len(Y_left)!=0:\n",
    "        entropy_after_split += ( np.float(len(Y_left)/len(Y)) * entropy_node(Y_left) ) \n",
    "    if len(Y_right) !=0 :\n",
    "        entropy_after_split += ( np.float(len(Y_right)/len(Y)) * entropy_node(Y_right) )\n",
    "            \n",
    "    return entropy_after_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate split_info of feature having index f_i\n",
    "\n",
    "def find_split_info(X, Y, f_i, boundry):\n",
    "    size = len(Y)\n",
    "    split_info = np.float(0)\n",
    "    \n",
    "    # feature on which we goona split\n",
    "    feature = X[:, f_i] # feature corresponds to index f_i\n",
    "    Y_left = Y[np.where(feature <= boundry)]\n",
    "    Y_right = Y[np.where(feature > boundry)]\n",
    "    \n",
    "    if len(Y_left) != 0:\n",
    "        split_info += np.float( -1*np.float(len(Y_left)/size) * np.log2(len(Y_left)/size) ) \n",
    "    if len(Y_right) != 0:\n",
    "        split_info += np.float( -1*np.float(len(Y_right)/size) * np.log2(len(Y_right)/size) )\n",
    "        \n",
    "    return split_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTree(X, Y, features, level=0):\n",
    "    # Print some info as required\n",
    "    print(\"Level\",level) \n",
    "    \n",
    "    # store different classes possible with its frequency\n",
    "    class_freq = {}\n",
    "    for c in Y:\n",
    "        class_freq[c] = class_freq.get(c,0) + 1\n",
    "    \n",
    "    for c in class_freq:\n",
    "        if class_freq[c] != 0:\n",
    "            print(\"Count of\",c,\"=\",class_freq[c])\n",
    "    \n",
    "    # Find current entropy\n",
    "    entropy_current = entropy_node(Y)\n",
    "    \n",
    "    \n",
    "    # Base case\n",
    "    # If node is pure, \n",
    "    if len(set(Y))==1 :\n",
    "        print(\"Current Entropy is = 0.0\")\n",
    "        print(\"Reached Leaf Node\")\n",
    "    \n",
    "    # If no feature left to split\n",
    "    elif len(features) == 0:\n",
    "        print(\"Current Entropy is =\",entropy_current)\n",
    "        print(\"Reached Leaf Node\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Current Entropy is =\",entropy_current)\n",
    "        \n",
    "        # Find max info_gain\n",
    "        max_gain_ratio = 0\n",
    "        max_gain_ratio_feature = features[0]\n",
    "        X_feature_boundry = -1 # initially, we change this inside loop\n",
    "        \n",
    "        # check info_gain for each feature\n",
    "        for f in features:\n",
    "            '''\n",
    "                Since in Iris dataset all features have continuous data. So we need to find a boundry\n",
    "                To find an boundry we first sort feature data, and then try boundry as middle value b/w 2 consecutive data\n",
    "            '''\n",
    "            X_feature = X[:, feature_indices[f]]\n",
    "            X_feature.sort()\n",
    "            #X_feature_boundry = X_feature[0]/2 # Assume for now, It may be changed below\n",
    "            #max_gain_ratio = info_fi(X, Y, feature_indices[f], X_feature_boundry) / find_split_info(X, Y, feature_indices[f], X_feature_boundry)\n",
    "            \n",
    "            # Try different boundries\n",
    "            X_len = len(X_feature)\n",
    "            if X_len == 1:\n",
    "                print(\"len is 1\")\n",
    "                continue\n",
    "                \n",
    "            for i in range(X_len-1):\n",
    "                curr_boundry = np.float(X_feature[i]+X_feature[i+1])/2\n",
    "                info_after_split_f = info_fi(X, Y, feature_indices[f], curr_boundry)\n",
    "                info_gain_f = entropy_current - info_after_split_f\n",
    "                split_info_f = find_split_info(X, Y, feature_indices[f], curr_boundry)\n",
    "                gain_ratio_f = np.float(info_gain_f/split_info_f)\n",
    "            \n",
    "                if gain_ratio_f>max_gain_ratio:\n",
    "                    max_gain_ratio = gain_ratio_f\n",
    "                    max_gain_ratio_feature = f\n",
    "                    X_feature_boundry = curr_boundry\n",
    "        \n",
    "        print(\"Splitting on feature\",max_gain_ratio_feature, \"<=\",X_feature_boundry, \"with gain ratio\", max_gain_ratio)\n",
    "        \n",
    "        # Now split on basis of max_gain_ratio_feature and X_feature_boundry and call on two sides of boundry\n",
    "        # 1st remove this max_gain_ratio_feature from current features list\n",
    "        new_features = []\n",
    "        for f in features:\n",
    "            if f != max_gain_ratio_feature:\n",
    "                new_features.append(f)\n",
    "                \n",
    "        f_i = feature_indices[max_gain_ratio_feature] # bcz this index(f_i) gives us the exact column correspond to required featue \n",
    "        f_i_feature = X[:, f_i] # feature corresponds to index f_i\n",
    "        \n",
    "        # Recursive calls on 2 sides of boundry\n",
    "        X_left = X[np.where(f_i_feature <= X_feature_boundry)]\n",
    "        Y_left = Y[np.where(f_i_feature <= X_feature_boundry)]\n",
    "        print()\n",
    "        DecisionTree(X_left, Y_left, new_features, level+1)\n",
    "        \n",
    "        X_right = X[np.where(f_i_feature > X_feature_boundry)]\n",
    "        Y_right = Y[np.where(f_i_feature > X_feature_boundry)]\n",
    "        print()\n",
    "        DecisionTree(X_right, Y_right, new_features, level+1)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0\n",
      "Count of 0 = 50\n",
      "Count of 1 = 50\n",
      "Count of 2 = 50\n",
      "Current Entropy is = 1.584962500721156\n",
      "Splitting on feature petal length (cm) <= 1.9 with gain ratio 0.9999999999999999\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 50\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of 1 = 50\n",
      "Count of 2 = 50\n",
      "Current Entropy is = 1.0\n",
      "Splitting on feature sepal length (cm) <= 6.2 with gain ratio 0.929259315921287\n",
      "\n",
      "Level 2\n",
      "Count of 1 = 49\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level 2\n",
      "Count of 1 = 1\n",
      "Count of 2 = 50\n",
      "Current Entropy is = 0.13923299905509887\n",
      "Splitting on feature petal width (cm) <= 1.6 with gain ratio 0.26402404255823636\n",
      "\n",
      "Level 3\n",
      "Count of 1 = 1\n",
      "Count of 2 = 2\n",
      "Current Entropy is = 0.9182958340544896\n",
      "Splitting on feature sepal width (cm) <= -1 with gain ratio 0\n",
      "\n",
      "Level 4\n",
      "Current Entropy is = 0\n",
      "Reached Leaf Node\n",
      "\n",
      "Level 4\n",
      "Count of 1 = 1\n",
      "Count of 2 = 2\n",
      "Current Entropy is = 0.9182958340544896\n",
      "Reached Leaf Node\n",
      "\n",
      "Level 3\n",
      "Count of 2 = 48\n",
      "Current Entropy is = 0.0\n",
      "Reached Leaf Node\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mehak\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\ipykernel_launcher.py:59: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "DecisionTree(X,Y,features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.float(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
